{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The objective is to  solve using sklearn annd then use self-developed code for solving the same problem and compare the performance-time for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data to the notebook\n",
    "data = pd.read_csv('data\\\\dataq2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.347926</td>\n",
       "      <td>0.862573</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.145161</td>\n",
       "      <td>0.628655</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.034562</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.149770</td>\n",
       "      <td>0.073099</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.352535</td>\n",
       "      <td>-0.160819</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-0.306452</td>\n",
       "      <td>-0.687134</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>-0.006912</td>\n",
       "      <td>-0.464912</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>-0.080645</td>\n",
       "      <td>-0.236842</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.126728</td>\n",
       "      <td>-0.581871</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-0.066820</td>\n",
       "      <td>-0.921053</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          x1        x2    y\n",
       "0  -0.347926  0.862573  0.0\n",
       "1  -0.145161  0.628655  0.0\n",
       "2  -0.034562  0.289474  0.0\n",
       "3  -0.149770  0.073099  0.0\n",
       "4  -0.352535 -0.160819  0.0\n",
       "..       ...       ...  ...\n",
       "64 -0.306452 -0.687134  1.0\n",
       "65 -0.006912 -0.464912  0.0\n",
       "66 -0.080645 -0.236842  1.0\n",
       "67  0.126728 -0.581871  1.0\n",
       "68 -0.066820 -0.921053  0.0\n",
       "\n",
       "[69 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['y']\n",
    "X = data.drop(columns='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, alpha=0.0001,\n",
    "                    solver='adam', verbose=10, random_state=42, tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69542417\n",
      "Iteration 2, loss = 0.68761054\n",
      "Iteration 3, loss = 0.67995604\n",
      "Iteration 4, loss = 0.67247208\n",
      "Iteration 5, loss = 0.66521131\n",
      "Iteration 6, loss = 0.65832046\n",
      "Iteration 7, loss = 0.65189087\n",
      "Iteration 8, loss = 0.64576167\n",
      "Iteration 9, loss = 0.63986679\n",
      "Iteration 10, loss = 0.63418372\n",
      "Iteration 11, loss = 0.62868574\n",
      "Iteration 12, loss = 0.62340085\n",
      "Iteration 13, loss = 0.61844033\n",
      "Iteration 14, loss = 0.61367426\n",
      "Iteration 15, loss = 0.60903743\n",
      "Iteration 16, loss = 0.60460791\n",
      "Iteration 17, loss = 0.60024886\n",
      "Iteration 18, loss = 0.59587572\n",
      "Iteration 19, loss = 0.59163930\n",
      "Iteration 20, loss = 0.58747793\n",
      "Iteration 21, loss = 0.58338829\n",
      "Iteration 22, loss = 0.57938970\n",
      "Iteration 23, loss = 0.57545137\n",
      "Iteration 24, loss = 0.57157590\n",
      "Iteration 25, loss = 0.56780785\n",
      "Iteration 26, loss = 0.56414730\n",
      "Iteration 27, loss = 0.56055752\n",
      "Iteration 28, loss = 0.55711361\n",
      "Iteration 29, loss = 0.55378731\n",
      "Iteration 30, loss = 0.55054069\n",
      "Iteration 31, loss = 0.54738684\n",
      "Iteration 32, loss = 0.54431406\n",
      "Iteration 33, loss = 0.54130836\n",
      "Iteration 34, loss = 0.53837664\n",
      "Iteration 35, loss = 0.53551026\n",
      "Iteration 36, loss = 0.53273965\n",
      "Iteration 37, loss = 0.53005255\n",
      "Iteration 38, loss = 0.52744889\n",
      "Iteration 39, loss = 0.52494266\n",
      "Iteration 40, loss = 0.52255915\n",
      "Iteration 41, loss = 0.52026062\n",
      "Iteration 42, loss = 0.51801677\n",
      "Iteration 43, loss = 0.51584613\n",
      "Iteration 44, loss = 0.51373211\n",
      "Iteration 45, loss = 0.51167409\n",
      "Iteration 46, loss = 0.50967155\n",
      "Iteration 47, loss = 0.50771456\n",
      "Iteration 48, loss = 0.50579367\n",
      "Iteration 49, loss = 0.50391735\n",
      "Iteration 50, loss = 0.50210950\n",
      "Iteration 51, loss = 0.50034671\n",
      "Iteration 52, loss = 0.49862042\n",
      "Iteration 53, loss = 0.49692188\n",
      "Iteration 54, loss = 0.49526406\n",
      "Iteration 55, loss = 0.49365635\n",
      "Iteration 56, loss = 0.49208401\n",
      "Iteration 57, loss = 0.49054571\n",
      "Iteration 58, loss = 0.48902109\n",
      "Iteration 59, loss = 0.48753708\n",
      "Iteration 60, loss = 0.48608211\n",
      "Iteration 61, loss = 0.48466152\n",
      "Iteration 62, loss = 0.48327237\n",
      "Iteration 63, loss = 0.48190107\n",
      "Iteration 64, loss = 0.48054556\n",
      "Iteration 65, loss = 0.47920838\n",
      "Iteration 66, loss = 0.47789333\n",
      "Iteration 67, loss = 0.47664201\n",
      "Iteration 68, loss = 0.47543047\n",
      "Iteration 69, loss = 0.47422928\n",
      "Iteration 70, loss = 0.47303301\n",
      "Iteration 71, loss = 0.47183586\n",
      "Iteration 72, loss = 0.47064144\n",
      "Iteration 73, loss = 0.46945497\n",
      "Iteration 74, loss = 0.46827098\n",
      "Iteration 75, loss = 0.46710276\n",
      "Iteration 76, loss = 0.46595974\n",
      "Iteration 77, loss = 0.46484744\n",
      "Iteration 78, loss = 0.46375615\n",
      "Iteration 79, loss = 0.46267299\n",
      "Iteration 80, loss = 0.46159276\n",
      "Iteration 81, loss = 0.46050400\n",
      "Iteration 82, loss = 0.45940829\n",
      "Iteration 83, loss = 0.45832362\n",
      "Iteration 84, loss = 0.45724895\n",
      "Iteration 85, loss = 0.45617274\n",
      "Iteration 86, loss = 0.45509157\n",
      "Iteration 87, loss = 0.45403074\n",
      "Iteration 88, loss = 0.45296571\n",
      "Iteration 89, loss = 0.45189608\n",
      "Iteration 90, loss = 0.45082239\n",
      "Iteration 91, loss = 0.44975186\n",
      "Iteration 92, loss = 0.44870230\n",
      "Iteration 93, loss = 0.44767650\n",
      "Iteration 94, loss = 0.44664410\n",
      "Iteration 95, loss = 0.44561166\n",
      "Iteration 96, loss = 0.44457106\n",
      "Iteration 97, loss = 0.44354989\n",
      "Iteration 98, loss = 0.44254357\n",
      "Iteration 99, loss = 0.44152800\n",
      "Iteration 100, loss = 0.44050296\n",
      "Iteration 101, loss = 0.43948141\n",
      "Iteration 102, loss = 0.43847209\n",
      "Iteration 103, loss = 0.43747133\n",
      "Iteration 104, loss = 0.43649064\n",
      "Iteration 105, loss = 0.43553466\n",
      "Iteration 106, loss = 0.43456826\n",
      "Iteration 107, loss = 0.43359161\n",
      "Iteration 108, loss = 0.43261798\n",
      "Iteration 109, loss = 0.43165387\n",
      "Iteration 110, loss = 0.43069716\n",
      "Iteration 111, loss = 0.42974497\n",
      "Iteration 112, loss = 0.42879147\n",
      "Iteration 113, loss = 0.42783261\n",
      "Iteration 114, loss = 0.42686512\n",
      "Iteration 115, loss = 0.42589798\n",
      "Iteration 116, loss = 0.42492843\n",
      "Iteration 117, loss = 0.42396438\n",
      "Iteration 118, loss = 0.42300830\n",
      "Iteration 119, loss = 0.42206206\n",
      "Iteration 120, loss = 0.42110877\n",
      "Iteration 121, loss = 0.42014348\n",
      "Iteration 122, loss = 0.41918947\n",
      "Iteration 123, loss = 0.41824083\n",
      "Iteration 124, loss = 0.41728754\n",
      "Iteration 125, loss = 0.41632257\n",
      "Iteration 126, loss = 0.41534812\n",
      "Iteration 127, loss = 0.41437938\n",
      "Iteration 128, loss = 0.41340649\n",
      "Iteration 129, loss = 0.41243209\n",
      "Iteration 130, loss = 0.41145961\n",
      "Iteration 131, loss = 0.41051322\n",
      "Iteration 132, loss = 0.40957943\n",
      "Iteration 133, loss = 0.40862557\n",
      "Iteration 134, loss = 0.40765870\n",
      "Iteration 135, loss = 0.40669267\n",
      "Iteration 136, loss = 0.40573654\n",
      "Iteration 137, loss = 0.40478050\n",
      "Iteration 138, loss = 0.40382595\n",
      "Iteration 139, loss = 0.40286772\n",
      "Iteration 140, loss = 0.40191435\n",
      "Iteration 141, loss = 0.40096500\n",
      "Iteration 142, loss = 0.40001279\n",
      "Iteration 143, loss = 0.39907273\n",
      "Iteration 144, loss = 0.39813157\n",
      "Iteration 145, loss = 0.39718800\n",
      "Iteration 146, loss = 0.39623378\n",
      "Iteration 147, loss = 0.39530041\n",
      "Iteration 148, loss = 0.39438479\n",
      "Iteration 149, loss = 0.39346595\n",
      "Iteration 150, loss = 0.39253785\n",
      "Iteration 151, loss = 0.39159819\n",
      "Iteration 152, loss = 0.39067953\n",
      "Iteration 153, loss = 0.38977107\n",
      "Iteration 154, loss = 0.38884969\n",
      "Iteration 155, loss = 0.38794204\n",
      "Iteration 156, loss = 0.38704355\n",
      "Iteration 157, loss = 0.38615821\n",
      "Iteration 158, loss = 0.38530013\n",
      "Iteration 159, loss = 0.38442827\n",
      "Iteration 160, loss = 0.38356785\n",
      "Iteration 161, loss = 0.38270930\n",
      "Iteration 162, loss = 0.38185466\n",
      "Iteration 163, loss = 0.38099476\n",
      "Iteration 164, loss = 0.38013055\n",
      "Iteration 165, loss = 0.37929494\n",
      "Iteration 166, loss = 0.37845435\n",
      "Iteration 167, loss = 0.37761288\n",
      "Iteration 168, loss = 0.37677430\n",
      "Iteration 169, loss = 0.37596322\n",
      "Iteration 170, loss = 0.37516783\n",
      "Iteration 171, loss = 0.37435873\n",
      "Iteration 172, loss = 0.37355239\n",
      "Iteration 173, loss = 0.37277082\n",
      "Iteration 174, loss = 0.37197973\n",
      "Iteration 175, loss = 0.37120444\n",
      "Iteration 176, loss = 0.37041935\n",
      "Iteration 177, loss = 0.36962721\n",
      "Iteration 178, loss = 0.36885864\n",
      "Iteration 179, loss = 0.36810475\n",
      "Iteration 180, loss = 0.36736767\n",
      "Iteration 181, loss = 0.36659801\n",
      "Iteration 182, loss = 0.36585597\n",
      "Iteration 183, loss = 0.36513264\n",
      "Iteration 184, loss = 0.36442361\n",
      "Iteration 185, loss = 0.36371430\n",
      "Iteration 186, loss = 0.36298251\n",
      "Iteration 187, loss = 0.36224554\n",
      "Iteration 188, loss = 0.36152781\n",
      "Iteration 189, loss = 0.36082860\n",
      "Iteration 190, loss = 0.36013311\n",
      "Iteration 191, loss = 0.35941222\n",
      "Iteration 192, loss = 0.35870556\n",
      "Iteration 193, loss = 0.35800655\n",
      "Iteration 194, loss = 0.35729835\n",
      "Iteration 195, loss = 0.35658537\n",
      "Iteration 196, loss = 0.35587000\n",
      "Iteration 197, loss = 0.35518385\n",
      "Iteration 198, loss = 0.35449569\n",
      "Iteration 199, loss = 0.35380285\n",
      "Iteration 200, loss = 0.35311188\n",
      "Iteration 201, loss = 0.35246409\n",
      "Iteration 202, loss = 0.35180296\n",
      "Iteration 203, loss = 0.35110265\n",
      "Iteration 204, loss = 0.35040491\n",
      "Iteration 205, loss = 0.34974861\n",
      "Iteration 206, loss = 0.34909317\n",
      "Iteration 207, loss = 0.34843238\n",
      "Iteration 208, loss = 0.34774813\n",
      "Iteration 209, loss = 0.34708022\n",
      "Iteration 210, loss = 0.34644772\n",
      "Iteration 211, loss = 0.34580173\n",
      "Iteration 212, loss = 0.34513269\n",
      "Iteration 213, loss = 0.34449258\n",
      "Iteration 214, loss = 0.34382399\n",
      "Iteration 215, loss = 0.34314732\n",
      "Iteration 216, loss = 0.34251514\n",
      "Iteration 217, loss = 0.34188635\n",
      "Iteration 218, loss = 0.34124324\n",
      "Iteration 219, loss = 0.34060191\n",
      "Iteration 220, loss = 0.33995440\n",
      "Iteration 221, loss = 0.33932186\n",
      "Iteration 222, loss = 0.33869243\n",
      "Iteration 223, loss = 0.33804984\n",
      "Iteration 224, loss = 0.33743401\n",
      "Iteration 225, loss = 0.33684851\n",
      "Iteration 226, loss = 0.33627135\n",
      "Iteration 227, loss = 0.33565840\n",
      "Iteration 228, loss = 0.33507841\n",
      "Iteration 229, loss = 0.33447248\n",
      "Iteration 230, loss = 0.33385066\n",
      "Iteration 231, loss = 0.33324780\n",
      "Iteration 232, loss = 0.33265692\n",
      "Iteration 233, loss = 0.33203383\n",
      "Iteration 234, loss = 0.33142593\n",
      "Iteration 235, loss = 0.33084330\n",
      "Iteration 236, loss = 0.33023237\n",
      "Iteration 237, loss = 0.32962543\n",
      "Iteration 238, loss = 0.32902370\n",
      "Iteration 239, loss = 0.32844602\n",
      "Iteration 240, loss = 0.32783992\n",
      "Iteration 241, loss = 0.32723769\n",
      "Iteration 242, loss = 0.32661043\n",
      "Iteration 243, loss = 0.32601904\n",
      "Iteration 244, loss = 0.32545256\n",
      "Iteration 245, loss = 0.32482369\n",
      "Iteration 246, loss = 0.32419006\n",
      "Iteration 247, loss = 0.32361404\n",
      "Iteration 248, loss = 0.32302503\n",
      "Iteration 249, loss = 0.32243130\n",
      "Iteration 250, loss = 0.32184090\n",
      "Iteration 251, loss = 0.32125134\n",
      "Iteration 252, loss = 0.32067460\n",
      "Iteration 253, loss = 0.32008278\n",
      "Iteration 254, loss = 0.31947944\n",
      "Iteration 255, loss = 0.31887298\n",
      "Iteration 256, loss = 0.31826799\n",
      "Iteration 257, loss = 0.31765657\n",
      "Iteration 258, loss = 0.31705152\n",
      "Iteration 259, loss = 0.31647869\n",
      "Iteration 260, loss = 0.31585811\n",
      "Iteration 261, loss = 0.31524076\n",
      "Iteration 262, loss = 0.31465306\n",
      "Iteration 263, loss = 0.31403185\n",
      "Iteration 264, loss = 0.31348951\n",
      "Iteration 265, loss = 0.31290158\n",
      "Iteration 266, loss = 0.31227546\n",
      "Iteration 267, loss = 0.31168192\n",
      "Iteration 268, loss = 0.31108726\n",
      "Iteration 269, loss = 0.31048178\n",
      "Iteration 270, loss = 0.30991737\n",
      "Iteration 271, loss = 0.30929031\n",
      "Iteration 272, loss = 0.30864284\n",
      "Iteration 273, loss = 0.30804029\n",
      "Iteration 274, loss = 0.30742064\n",
      "Iteration 275, loss = 0.30678903\n",
      "Iteration 276, loss = 0.30619307\n",
      "Iteration 277, loss = 0.30558464\n",
      "Iteration 278, loss = 0.30493235\n",
      "Iteration 279, loss = 0.30427336\n",
      "Iteration 280, loss = 0.30366768\n",
      "Iteration 281, loss = 0.30304393\n",
      "Iteration 282, loss = 0.30241051\n",
      "Iteration 283, loss = 0.30176986\n",
      "Iteration 284, loss = 0.30114941\n",
      "Iteration 285, loss = 0.30053600\n",
      "Iteration 286, loss = 0.29987856\n",
      "Iteration 287, loss = 0.29924805\n",
      "Iteration 288, loss = 0.29864358\n",
      "Iteration 289, loss = 0.29805406\n",
      "Iteration 290, loss = 0.29746712\n",
      "Iteration 291, loss = 0.29681668\n",
      "Iteration 292, loss = 0.29617577\n",
      "Iteration 293, loss = 0.29555240\n",
      "Iteration 294, loss = 0.29496172\n",
      "Iteration 295, loss = 0.29436459\n",
      "Iteration 296, loss = 0.29369236\n",
      "Iteration 297, loss = 0.29303385\n",
      "Iteration 298, loss = 0.29244649\n",
      "Iteration 299, loss = 0.29183576\n",
      "Iteration 300, loss = 0.29117752\n",
      "Iteration 301, loss = 0.29054268\n",
      "Iteration 302, loss = 0.28988476\n",
      "Iteration 303, loss = 0.28929503\n",
      "Iteration 304, loss = 0.28867141\n",
      "Iteration 305, loss = 0.28804462\n",
      "Iteration 306, loss = 0.28737151\n",
      "Iteration 307, loss = 0.28672759\n",
      "Iteration 308, loss = 0.28609678\n",
      "Iteration 309, loss = 0.28546228\n",
      "Iteration 310, loss = 0.28478389\n",
      "Iteration 311, loss = 0.28413088\n",
      "Iteration 312, loss = 0.28348432\n",
      "Iteration 313, loss = 0.28285777\n",
      "Iteration 314, loss = 0.28216941\n",
      "Iteration 315, loss = 0.28153156\n",
      "Iteration 316, loss = 0.28091816\n",
      "Iteration 317, loss = 0.28024762\n",
      "Iteration 318, loss = 0.27959880\n",
      "Iteration 319, loss = 0.27892344\n",
      "Iteration 320, loss = 0.27830169\n",
      "Iteration 321, loss = 0.27764225\n",
      "Iteration 322, loss = 0.27696338\n",
      "Iteration 323, loss = 0.27630900\n",
      "Iteration 324, loss = 0.27567478\n",
      "Iteration 325, loss = 0.27501179\n",
      "Iteration 326, loss = 0.27433643\n",
      "Iteration 327, loss = 0.27367084\n",
      "Iteration 328, loss = 0.27304631\n",
      "Iteration 329, loss = 0.27237627\n",
      "Iteration 330, loss = 0.27166669\n",
      "Iteration 331, loss = 0.27104023\n",
      "Iteration 332, loss = 0.27035991\n",
      "Iteration 333, loss = 0.26971671\n",
      "Iteration 334, loss = 0.26903113\n",
      "Iteration 335, loss = 0.26839650\n",
      "Iteration 336, loss = 0.26773357\n",
      "Iteration 337, loss = 0.26708377\n",
      "Iteration 338, loss = 0.26635330\n",
      "Iteration 339, loss = 0.26573650\n",
      "Iteration 340, loss = 0.26510064\n",
      "Iteration 341, loss = 0.26437130\n",
      "Iteration 342, loss = 0.26370462\n",
      "Iteration 343, loss = 0.26301392\n",
      "Iteration 344, loss = 0.26238250\n",
      "Iteration 345, loss = 0.26167451\n",
      "Iteration 346, loss = 0.26097876\n",
      "Iteration 347, loss = 0.26038153\n",
      "Iteration 348, loss = 0.25970291\n",
      "Iteration 349, loss = 0.25898040\n",
      "Iteration 350, loss = 0.25829564\n",
      "Iteration 351, loss = 0.25767665\n",
      "Iteration 352, loss = 0.25696678\n",
      "Iteration 353, loss = 0.25630489\n",
      "Iteration 354, loss = 0.25562984\n",
      "Iteration 355, loss = 0.25498954\n",
      "Iteration 356, loss = 0.25429200\n",
      "Iteration 357, loss = 0.25358284\n",
      "Iteration 358, loss = 0.25294768\n",
      "Iteration 359, loss = 0.25226706\n",
      "Iteration 360, loss = 0.25155634\n",
      "Iteration 361, loss = 0.25085805\n",
      "Iteration 362, loss = 0.25019451\n",
      "Iteration 363, loss = 0.24955501\n",
      "Iteration 364, loss = 0.24883788\n",
      "Iteration 365, loss = 0.24817841\n",
      "Iteration 366, loss = 0.24753169\n",
      "Iteration 367, loss = 0.24686540\n",
      "Iteration 368, loss = 0.24618619\n",
      "Iteration 369, loss = 0.24550889\n",
      "Iteration 370, loss = 0.24482633\n",
      "Iteration 371, loss = 0.24422084\n",
      "Iteration 372, loss = 0.24351333\n",
      "Iteration 373, loss = 0.24281579\n",
      "Iteration 374, loss = 0.24215165\n",
      "Iteration 375, loss = 0.24151225\n",
      "Iteration 376, loss = 0.24079231\n",
      "Iteration 377, loss = 0.24016309\n",
      "Iteration 378, loss = 0.23951974\n",
      "Iteration 379, loss = 0.23879772\n",
      "Iteration 380, loss = 0.23817566\n",
      "Iteration 381, loss = 0.23750158\n",
      "Iteration 382, loss = 0.23678886\n",
      "Iteration 383, loss = 0.23613236\n",
      "Iteration 384, loss = 0.23548304\n",
      "Iteration 385, loss = 0.23482242\n",
      "Iteration 386, loss = 0.23415390\n",
      "Iteration 387, loss = 0.23348052\n",
      "Iteration 388, loss = 0.23277729\n",
      "Iteration 389, loss = 0.23211910\n",
      "Iteration 390, loss = 0.23147465\n",
      "Iteration 391, loss = 0.23079441\n",
      "Iteration 392, loss = 0.23013260\n",
      "Iteration 393, loss = 0.22943126\n",
      "Iteration 394, loss = 0.22878721\n",
      "Iteration 395, loss = 0.22814855\n",
      "Iteration 396, loss = 0.22746943\n",
      "Iteration 397, loss = 0.22682640\n",
      "Iteration 398, loss = 0.22618850\n",
      "Iteration 399, loss = 0.22554249\n",
      "Iteration 400, loss = 0.22486277\n",
      "Iteration 401, loss = 0.22420678\n",
      "Iteration 402, loss = 0.22353712\n",
      "Iteration 403, loss = 0.22289422\n",
      "Iteration 404, loss = 0.22225509\n",
      "Iteration 405, loss = 0.22161129\n",
      "Iteration 406, loss = 0.22089280\n",
      "Iteration 407, loss = 0.22020312\n",
      "Iteration 408, loss = 0.21956107\n",
      "Iteration 409, loss = 0.21895322\n",
      "Iteration 410, loss = 0.21828561\n",
      "Iteration 411, loss = 0.21762746\n",
      "Iteration 412, loss = 0.21701073\n",
      "Iteration 413, loss = 0.21632557\n",
      "Iteration 414, loss = 0.21566247\n",
      "Iteration 415, loss = 0.21504347\n",
      "Iteration 416, loss = 0.21440964\n",
      "Iteration 417, loss = 0.21379770\n",
      "Iteration 418, loss = 0.21310163\n",
      "Iteration 419, loss = 0.21241189\n",
      "Iteration 420, loss = 0.21181338\n",
      "Iteration 421, loss = 0.21119756\n",
      "Iteration 422, loss = 0.21058002\n",
      "Iteration 423, loss = 0.20992296\n",
      "Iteration 424, loss = 0.20928771\n",
      "Iteration 425, loss = 0.20865042\n",
      "Iteration 426, loss = 0.20801096\n",
      "Iteration 427, loss = 0.20741722\n",
      "Iteration 428, loss = 0.20675295\n",
      "Iteration 429, loss = 0.20605271\n",
      "Iteration 430, loss = 0.20547282\n",
      "Iteration 431, loss = 0.20479828\n",
      "Iteration 432, loss = 0.20420642\n",
      "Iteration 433, loss = 0.20353494\n",
      "Iteration 434, loss = 0.20290696\n",
      "Iteration 435, loss = 0.20228545\n",
      "Iteration 436, loss = 0.20163588\n",
      "Iteration 437, loss = 0.20099996\n",
      "Iteration 438, loss = 0.20042671\n",
      "Iteration 439, loss = 0.19977421\n",
      "Iteration 440, loss = 0.19917804\n",
      "Iteration 441, loss = 0.19854429\n",
      "Iteration 442, loss = 0.19795899\n",
      "Iteration 443, loss = 0.19734679\n",
      "Iteration 444, loss = 0.19668182\n",
      "Iteration 445, loss = 0.19603483\n",
      "Iteration 446, loss = 0.19546426\n",
      "Iteration 447, loss = 0.19482651\n",
      "Iteration 448, loss = 0.19419798\n",
      "Iteration 449, loss = 0.19359725\n",
      "Iteration 450, loss = 0.19295593\n",
      "Iteration 451, loss = 0.19235905\n",
      "Iteration 452, loss = 0.19180011\n",
      "Iteration 453, loss = 0.19115031\n",
      "Iteration 454, loss = 0.19055213\n",
      "Iteration 455, loss = 0.18991857\n",
      "Iteration 456, loss = 0.18929701\n",
      "Iteration 457, loss = 0.18866166\n",
      "Iteration 458, loss = 0.18806827\n",
      "Iteration 459, loss = 0.18747335\n",
      "Iteration 460, loss = 0.18690269\n",
      "Iteration 461, loss = 0.18628036\n",
      "Iteration 462, loss = 0.18561110\n",
      "Iteration 463, loss = 0.18493928\n",
      "Iteration 464, loss = 0.18436603\n",
      "Iteration 465, loss = 0.18372443\n",
      "Iteration 466, loss = 0.18306343\n",
      "Iteration 467, loss = 0.18245378\n",
      "Iteration 468, loss = 0.18193172\n",
      "Iteration 469, loss = 0.18135170\n",
      "Iteration 470, loss = 0.18074423\n",
      "Iteration 471, loss = 0.18020680\n",
      "Iteration 472, loss = 0.17956694\n",
      "Iteration 473, loss = 0.17896884\n",
      "Iteration 474, loss = 0.17839109\n",
      "Iteration 475, loss = 0.17778366\n",
      "Iteration 476, loss = 0.17720757\n",
      "Iteration 477, loss = 0.17660271\n",
      "Iteration 478, loss = 0.17602185\n",
      "Iteration 479, loss = 0.17542236\n",
      "Iteration 480, loss = 0.17483341\n",
      "Iteration 481, loss = 0.17424982\n",
      "Iteration 482, loss = 0.17366973\n",
      "Iteration 483, loss = 0.17307835\n",
      "Iteration 484, loss = 0.17252964\n",
      "Iteration 485, loss = 0.17195076\n",
      "Iteration 486, loss = 0.17131125\n",
      "Iteration 487, loss = 0.17069301\n",
      "Iteration 488, loss = 0.17013110\n",
      "Iteration 489, loss = 0.16955159\n",
      "Iteration 490, loss = 0.16894758\n",
      "Iteration 491, loss = 0.16836067\n",
      "Iteration 492, loss = 0.16776422\n",
      "Iteration 493, loss = 0.16716845\n",
      "Iteration 494, loss = 0.16653624\n",
      "Iteration 495, loss = 0.16592757\n",
      "Iteration 496, loss = 0.16532371\n",
      "Iteration 497, loss = 0.16475472\n",
      "Iteration 498, loss = 0.16412046\n",
      "Iteration 499, loss = 0.16351008\n",
      "Iteration 500, loss = 0.16289401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gusta\\miniconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42,\n",
       "              verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42,\n",
       "              verbose=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42,\n",
       "              verbose=10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test_scaled)# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
